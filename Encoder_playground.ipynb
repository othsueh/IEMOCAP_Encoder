{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "from transformers import AutoProcessor, HubertModel\n",
    "# from transformers import AutoFeatureExtractor, AutoImageProcessor, AutoModel\n",
    "# from PIL import Image\n",
    "# from matplotlib import pyplot as plt\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96480,)\n",
      "(70645,)\n",
      "(35520,)\n",
      "(29880,)\n",
      "(100960,)\n",
      "(28320,)\n",
      "(210321,)\n",
      "(72971,)\n",
      "(105440,)\n",
      "(147840,)\n"
     ]
    }
   ],
   "source": [
    "with open('../IEMOCAP/data_collected.pickle', 'rb') as file:\n",
    "    dataset = pickle.load(file)\n",
    "audio_frames = []\n",
    "for data in dataset[:10]:\n",
    "    audio_path = data[\"audio\"][\"audio_path\"]\n",
    "    audio_data = np.load(audio_path)\n",
    "    print(audio_data.shape)\n",
    "    audio_frames.append(audio_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioEncoder:\n",
    "    def __init__(self, model_name=\"facebook/hubert-large-ls960-ft\"):\n",
    "        self.model = HubertModel.from_pretrained(model_name)\n",
    "        self.processor = AutoProcessor.from_pretrained(model_name)\n",
    "\n",
    "    def encode(self, audio_data):\n",
    "        inputs = self.processor(audio_data, return_tensors=\"pt\", sampling_rate=16000, padding=\"longest\")\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        return outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/hubert-large-ls960-ft were not used when initializing HubertModel: ['lm_head.bias', 'lm_head.weight']\n",
      "- This IS expected if you are initializing HubertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing HubertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "audio_encoder = AudioEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 301, 1024])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_features = []\n",
    "for audio in audio_frames:\n",
    "    audio = torch.tensor(audio).cuda()\n",
    "    audio_features.append(audio_encoder.encode(audio))\n",
    "audio_features[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_test = audio_features[0].mean(dim=(0,1))\n",
    "audio_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_location = '../IEMOCAP/maeV_features/Ses01F_impro02_M016.npy'\n",
    "feature2_location2 = '../../Emotion-LLaMA/MERR/maeV_399_UTT/sample_00000005.npy'\n",
    "feature1 = np.load(feature_location)\n",
    "feature2 = np.load(feature2_location2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_to_tensor(data):\n",
    "    # Ensure we have exactly 16 frames by sampling or padding\n",
    "    target_frames = 16\n",
    "    total_frames = data.shape[0]\n",
    "    \n",
    "    if total_frames >= target_frames:\n",
    "        # Sample frames evenly\n",
    "        indices = np.linspace(0, total_frames-1, target_frames, dtype=int)\n",
    "        data = data[indices]\n",
    "    else:\n",
    "        # Pad with zeros if we have fewer frames\n",
    "        padding = np.zeros((target_frames - total_frames, *data.shape[1:]), dtype=data.dtype)\n",
    "        data = np.concatenate([data, padding], axis=0)\n",
    "    \n",
    "    # Convert to tensor and resize to 224x224\n",
    "    tensor_frames = torch.from_numpy(data).float()\n",
    "    tensor_frames = tensor_frames / 255.0  # Normalize to [0, 1]\n",
    "    if torch.cuda.is_available():\n",
    "        tensor_frames = tensor_frames.cuda()\n",
    "    \n",
    "    # Reshape to [B, C, T, H, W] format and resize\n",
    "    tensor_frames = tensor_frames.permute(0, 3, 1, 2)  # [B, C, H, W]\n",
    "    tensor_frames = torch.nn.functional.interpolate(\n",
    "        tensor_frames,\n",
    "        size=(224, 224),\n",
    "        mode='bilinear',\n",
    "        align_corners=False\n",
    "    )\n",
    "    \n",
    "    return list(tensor_frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAE Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoLocalEncoder:\n",
    "    def __init__(self, pretrained_model=\"facebook/vit-mae-large\"):\n",
    "        self.image_processor = AutoImageProcessor.from_pretrained(pretrained_model)\n",
    "        self.model = AutoModel.from_pretrained(pretrained_model)\n",
    "    @torch.no_grad()\n",
    "    def encode_video(self, video):\n",
    "        inputs = self.image_processor(video, return_tensors=\"pt\")\n",
    "        outputs = self.model(**inputs)\n",
    "        return outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_location = '../IEMOCAP/processed_videoframes/Ses01F_impro01_F005.npy' \n",
    "data = np.load(data_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = numpy_to_tensor(data)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_encoder = VideoLocalEncoder()\n",
    "feature = mae_encoder.encode_video(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 50, 1024])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024,)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_feature = feature.mean(dim=(0,1))\n",
    "n_feature.shape\n",
    "n_feature = n_feature.cpu().numpy()\n",
    "n_feature.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VideoMAE encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoLocalEncoder:\n",
    "    def __init__(self, pretrained_model=\"MCG-NJU/videomae-base-finetuned-kinetics\"):\n",
    "        self.feature_extractor = AutoFeatureExtractor.from_pretrained(pretrained_model)\n",
    "        self.model = AutoModel.from_pretrained(pretrained_model)\n",
    "    @torch.no_grad()\n",
    "    def encode_video(self, video):\n",
    "        inputs = self.feature_extractor(video, return_tensors=\"pt\")\n",
    "        outputs = self.model(**inputs)\n",
    "        return outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/datas/store163/othsueh/miniconda3/envs/llama/lib/python3.9/site-packages/transformers/models/videomae/feature_extraction_videomae.py:28: FutureWarning: The class VideoMAEFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use VideoMAEImageProcessor instead.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at MCG-NJU/videomae-base-finetuned-kinetics were not used when initializing VideoMAEModel: ['classifier.bias', 'classifier.weight', 'fc_norm.bias', 'fc_norm.weight']\n",
      "- This IS expected if you are initializing VideoMAEModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing VideoMAEModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "videoEncoder = VideoLocalEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1568, 768])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_data = numpy_to_tensor(data)\n",
    "# video = list(np.random.uniform(0,1,size=(16,3,224,224)))\n",
    "encoded_features = videoEncoder.encode_video(tensor_data)\n",
    "encoded_features.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
